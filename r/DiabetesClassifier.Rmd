---
title: "Diabetes Classifier"
output: html_notebook
---

#### Link to repository: https://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset

```{r}
# Load the dataset "diabetes_data_upload.csv"
Diabetes <- read.csv("~/Documents/github.git/DiabetesClassifier/data/diabetes_data_upload.csv", stringsAsFactors=TRUE)

# DF Names
names(Diabetes)

# DF Dimensions
dim(Diabetes)

# DF Summary 
summary(Diabetes)

# Have to check for N/A values (missing = FALSE in all instances)
# (Note: in the report they state that they removed 20 instances from 500, we have all 520)
missing = is.na(Diabetes) 
```


## Section I: Validation of classification models: Logistic Regression, Decision Tree, Random Forest
```{r}
# Load libraries
library(boot)
library(caret)
library(tree)
library(randomForest)
library(e1071)
    
# Set seed - generate same random numbers to reproduce results
set.seed(821)  

# Split data (80:20, training:testing)

  # sample( ) function randomly picks 80% rows from the data set (sampling without replacement)
  dt <- sort(sample(nrow(Diabetes), nrow(Diabetes)*.8))
  # Creates the training dataset with row numbers stored in dt
  train <- Diabetes[dt,] 
  # Creates the testing dataset excluding the row numbers mentioned in dt
  test <- Diabetes[-dt,] 
  
# Train control
train_control <- trainControl(method = "cv", number = 20)
```


### Logistic Regression K-Fold CV (K=20)
LR: a regression algorithm that predicts the value of a categorical variable. It finds the value of a variable that can only take two possible values (ex: pass or fail). Logistic regression finds the relationship between a categorical dependent variable and several independent variables. We can also state this as logistic regression finds the relationship between which class does a data object lies into and said object’s features. Not only does the algorithm find a class for an object, but it also gives justification and reasoning for that object to be in a specific class
```{r}
# Logistic Regression
cv.log <- train(class ~ ., data=train, trControl=train_control, method="glm", family=binomial())

# CV scores
summary(cv.log)

# Performance
mod.log <- glm(class ~ ., data=train, family=binomial(link="logit"))
pred.log <- predict(mod.log, newdata=test, type = "response")

# Prepare variables for CM
  # Check length
  length(pred.log)
  # Check contrasts
  contrasts(Diabetes$class)
  # Data input preparation
  probs.log = rep("Negative", 104)
  probs.log[pred.log >0.5] = "Positive"
  probs.log = as.factor(probs.log)

# Confusion matrix
confusionMatrix(data=probs.log, reference = test$class)

```
### Decision Tree K-Fold CV (K=20)
```{r}
# Decision Tree
cv.tree = train(class ~ ., 
                  data=train, 
                  method="rpart", 
                  trControl = train_control)

# cv.tree (check VarImp)

# Results
  mat.data <- c(
    mean(cv.tree$results$cp),
    mean(cv.tree$results$Accuracy),
    mean(cv.tree$results$AccuracySD),
    mean(cv.tree$results$Kappa),
    mean(cv.tree$results$KappaSD)
  )

  mat <- matrix(mat.data,byrow=FALSE)
  columns <-c("cp","Accuracy", "AccuracySD", "Kappa", "KappaSD")
  dimnames(mat) <- list(columns, "metrics")
  mat

# Performance
mod.tree = tree(class~., data=train)
plot(mod.tree)
text(mod.tree, pretty=0, cex=0.5)

# Prediction for CM
pred.tree = predict(mod.tree, newdata=test, type="class")

# Confusion matrix
confusionMatrix(data=pred.tree, reference = test$class)
```

### Random Forest K-Fold CV (K=20)
RF: the "forest" it builds, is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.
```{r}
# Random Forest
# CROSS VALIDATION

# Performance
mod.forest <- randomForest(class~., data=train, nodesize = 25, ntree = 200)

# Prediction for CM
pred.forest <- predict(mod.forest, newdata=test)

# Confusion matrix 
confusionMatrix(data=pred.forest, reference=test$class)
```

### Naive Bayes Classifier K-Fold CV (K=20)
NB: classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.  
A limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent (Are diabetes symptoms independent from each other?)
```{r}
# Naive Bayes 
# CROSS VALIDATION

# Performance
mod.nb <- naiveBayes(class~ ., data=train)
 
# Predicting on test data'
pred.nb <- predict(mod.nb, newdata=test)

# Confusion matrix
confusionMatrix(data=pred.nb, reference=test$class)
```


## Best model: X
```{r}
# Want to analyze best predictors for model

```

