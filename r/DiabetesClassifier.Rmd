---
title: "Diabetes Classifier"
output: html_notebook
date: April 20, 2022
---
&nbsp;

#### Link to repository: https://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset.

```{r}
# Load libraries
library(boot)
library(caret)
library(tree)
library(randomForest)
library(e1071)
library(rpart)
```

## Section I: Loading the data
```{r}
# Load the dataset "diabetes_data_upload.csv"
Diabetes <- read.csv("~/Documents/github.git/DiabetesClassifier/data/diabetes_data_upload.csv", stringsAsFactors=TRUE)

# DF Names
names(Diabetes)

# DF Dimensions
dim(Diabetes)

# DF Summary 
summary(Diabetes)

# Have to check for N/A values (missing = FALSE in all instances)
# (Note: in the report they state that they removed 20 instances from 500, we have all 520)
missing = is.na(Diabetes) 
```

## Section II: Data Distribution / Visualization
```{r}
# Age Distribution
histogram(Diabetes$Age, main="Histogram of Age Distribution", xlab="Age", ylab="Percentage")

# Sex Distribution
histogram(Diabetes$Sex, main="Histogram of Sex Distribution", xlab="Sex", ylab="Percentage")
  
# Save as png
  # Age
  png(file="Age-Distribution.png")
  histogram(Diabetes$Age, main="Histogram of Age Distribution", xlab="Age", ylab="Percentage")
  dev.off()
  # Sex
  png(file="Sex-Distribution.png")
  histogram(Diabetes$Sex, main="Histogram of Sex Distribution", xlab="Sex", ylab="Percentage")
  dev.off()
```

## Section III: Data Preparation
```{r}
# Set seed - generate same random numbers to reproduce results
set.seed(821)  

# Split data (80:20, training:testing)

  # sample( ) function randomly picks 80% rows from the data set (sampling without replacement)
  dt <- sort(sample(nrow(Diabetes), nrow(Diabetes)*.8))
  # Creates the training dataset with row numbers stored in dt
  train <- Diabetes[dt,] 
  # Creates the testing dataset excluding the row numbers mentioned in dt
  test <- Diabetes[-dt,] 
  
# Train Control definition (method we are going to use for CV)
train_control <- trainControl(method = "cv", number = 20)
```


## Section IV: Validation of classification models: Logistic Regression, Decision Tree, Random Forest, Naïve Bayes Classifier

### Logistic Regression K-Fold CV (K=20)
LR: a regression algorithm that predicts the value of a categorical variable. It finds the value of a variable that can only take two possible values (ex: pass or fail). Logistic regression finds the relationship between a categorical dependent variable and several independent variables. We can also state this as logistic regression finds the relationship between which class does a data object lies into and said object’s features. Not only does the algorithm find a class for an object, but it also gives justification and reasoning for that object to be in a specific class
```{r}
# Logistic Regression
cv.log <- train(class ~ ., 
                  data=train, 
                  trControl=train_control, 
                  method="glm", 
                  family=binomial())

summary(cv.log)

# Results
cv.log$results
  
# Variable Importance
var.log <-varImp(cv.log)
var.log
```

### Logistic Regression Performance (fit)
```{r}
# Performance
mod.log <- glm(class ~ ., data=train, family=binomial(link="logit"))
pred.log <- predict(mod.log, newdata=test, type = "response")

# Prepare variables for CM
  # Check length
  length(pred.log)
  # Check contrasts
  contrasts(Diabetes$class)
  # Data input preparation
  probs.log = rep("Negative", 104)
  probs.log[pred.log >0.5] = "Positive"
  probs.log = as.factor(probs.log)

# Confusion Matrix
confusionMatrix(data=probs.log, mode="everything", reference = test$class)
```

### Decision Tree K-Fold CV (K=20)
```{r}
# Cross-Validation
cv.tree = train(class ~ ., 
                  data=train, 
                  method="rpart", 
                  trControl = train_control)

# Results (optimal)
cv.tree

# Plot
plot(cv.tree)

# Results (mean)
mat.data <- c(
  mean(cv.tree$results$cp),
  mean(cv.tree$results$Accuracy),
  mean(cv.tree$results$AccuracySD),
  mean(cv.tree$results$Kappa),
  mean(cv.tree$results$KappaSD)
)

mat <- matrix(mat.data,byrow=FALSE)
columns <-c("Complexity Parameter","Accuracy", "AccuracySD", "Kappa", "KappaSD")
dimnames(mat) <- list(columns, "metrics")
mat
  
# Variable Importance
var.tree <- varImp(cv.tree)
var.tree
```

### Decision Tree Performance (fit)
```{r}
# Performance
mod.tree = tree(class~., data=train)
plot(mod.tree)
text(mod.tree, pretty=0, cex=0.5)

# Prediction for CM
pred.tree = predict(mod.tree, newdata=test, type="class")

# Confusion Matrix
confusionMatrix(data=pred.tree, mode="everything", reference = test$class)
```

### Random Forest K-Fold CV (K=20)
RF: the "forest" it builds, is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.
```{r}
# Cross-Validation
cv.rf <-  train(class ~ ., 
                  data=train, 
                  method="rf", 
                  trControl = train_control, tuneLength=15)

# Results
cv.rf

# Plot
plot(cv.rf)
  
# Variable Importance
var.rf <- varImp(cv.rf)
var.rf
```

### Random Forest Performance (fit)
```{r}
# Performance
mod.forest <- randomForest(class~., data=train, mtry=9) # ntree=500, nrnodes=77 

# Prediction for CM
pred.forest <- predict(mod.forest, newdata=test)

# Confusion Matrix 
confusionMatrix(data=pred.forest, mode="everything", reference=test$class)
```

### Naïve Bayes Classifier K-Fold CV (K=20)
NB: classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.  
A limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent (Are diabetes symptoms independent from each other?)
```{r}
# Cross-Validation
cv.nb <-  train(class~ ., 
                  data = train, 
                  method = "naive_bayes", 
                  )

# Results
cv.nb

# Plot
plot(cv.nb)

# Variable Importance
var.nb <- varImp(cv.nb)
var.nb
```

### Naïve Bayes Classifier Performance (fit)
```{r}
# Performance
mod.nb <- naiveBayes(class~ ., data=train, laplace=0, usekernel=FALSE, adjust=1)
 
# Prediction for CM
pred.nb <- predict(mod.nb, newdata=test)

# Confusion Matrix
confusionMatrix(data=pred.nb, mode="everything", reference=test$class)
```

## Section V: Analyze Variable Importance (first six)
```{r}
# Want to analyze best predictors for model (check varImp)
varOrder.log = var.log$importance[order(var.log$importance$Overall, decreasing=T), , drop=F]
varOrder.tree = var.tree$importance[order(var.tree$importance$Overall, decreasing=T), , drop=F]
varOrder.rf = var.rf$importance[order(var.rf$importance$Overall, decreasing=T), , drop=F]
varOrder.nb = var.nb$importance[order(var.nb$importance$Positive, decreasing=T), , drop=F]

head.log <- head(varOrder.log, 6)
head.tree <- head(varOrder.tree, 6)
head.rf <- head(varOrder.rf, 6)
head.nb <- data.frame(head(varOrder.nb$Positive, 6), row.names =c("Polyuria","Polydipsia","Sudden.weightloss", "Sex", "Partial.paresis", "Polyphagia")) # Needs 6 names

colnames(head.nb) <- c("Overall")

# View
par(mfrow=c(1,4))
head.log
head.tree
head.rf
head.nb

```

